{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lütfen kaynak belirtmeden bu içeriği tamamen veya kısmen kopyalamayın. Görüş, öneri, şikayet ve işbirliği gibi konular için **kolaydilisleme@gmail.com**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lemmatization\n",
    "\n",
    "Lemmatization'da stemming gibi kelimenin köküne ulaşmak için yapılan bir işlemdir. Stemming'den farkı olaya dilbilgisel olarak yaklaşmasıdır. Stemming gibi basitçe kelimenin sonundaki ekleri atmak yerine daha karmaşık işlemler gerçekleştirir. Bu nedenle, lemmatization genellikle daha doğru sonuçlar verir.\n",
    "\n",
    "NLTK kütüphanesinin içerisinden kullanabileceğimiz WordNetLemmatizer maalesef ki Türkçe için iyi çalışmıyor. Dolaysıyla Türkçe desteği olan farklı bir kütüphane kullancağız. Bu bağlamda Türkçe Lemmatization yapmak için olan [Zeyrek](https://github.com/obulat/zeyrek/) kütüphanesinde faydalanabiliriz.\n",
    "\n",
    "Ayrıca Türkçe NLP çalışmalarında çokça tercih edilen [Zemberek](https://github.com/ahmetaa/zemberek-nlp) ([python](https://github.com/loodos/zemberek-python)) kütüphanesi de kullanılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neden Lemmatization Yapılır?\n",
    "\n",
    "- **Anlamın Korunması:**  Lemmatization, kelimeleri köklerine indirgerken anlamın korunmasını sağlar. Örneğin, \"koşuyordum\", \"koşuyoruz\" ve \"koşacak\" kelimelerinin lemması \"koşmak\"tır. Bu, farklı çekim ekleri taşıyan kelimelerin temel anlamını koruyarak işlenmesine olanak tanır.\n",
    "\n",
    "- **Standarlaştırma:** Metinlerdeki kelime çeşitliliğini azaltarak, algoritmanın daha az ve daha anlamlı veri üzerinde çalışmasını sağlar. Bu, modelin eğitim süresini ve karmaşıklığını azaltabilir.\n",
    "\n",
    "- **Dilbilgisel İlişkilerin Anlaşılması:**  Lemmatization, kelimelerin dilbilgisel ilişkilerini anlamada yardımcı olur. Örneğin, \"yazdı\" kelimesinin lemması \"yaz\" olduğundan, bu kelimenin fiil olduğu ve geçmiş zamanla ilişkili olduğu anlaşılır.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming vs Lemmatization\n",
    "\n",
    "Stemming ve lemmatization arasındaki fark,\n",
    "\n",
    "- Stemming bağlamı bilmeksizin kelimeleri kesmesi nedeniyle daha hızlıdır, lemmatization ise işleme başlamadan önce kelimeleri bağlamsal olarak işlediği için daha yavaştır.\n",
    "\n",
    "- Stemming'de dönüştürülen temel kelimenin anlamlı olup olmaması değişkenlik gösterirken, lemmatization'da dönüştürülen temel kelimenin genelde düzgün bir anlamı vardır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install zeyrek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(koşmak_Verb)(-)(koş:verbRoot_S + uyor:vProgYor_S + vA3sg_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('koşuyor', ['koşmak'])]\n"
     ]
    }
   ],
   "source": [
    "import zeyrek\n",
    "\n",
    "analyzer = zeyrek.MorphAnalyzer()\n",
    "\n",
    "print(analyzer.lemmatize('koşuyor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/enes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# İngilizce örnek\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('running', pos=wordnet.VERB)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ön işleme sonrası lemmatization işlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APPENDING RESULT: <(Sokrates_Noun_Prop)(-)(sokrates:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(antik_Adj)(-)(antik:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(çağmak_Verb)(-)(çağ:verbRoot_S + vImp_S + ın:vA2pl_ST)>\n",
      "APPENDING RESULT: <(çağ_Noun)(-)(çağ:noun_S + a3sg_S + pnon_S + ın:gen_ST)>\n",
      "APPENDING RESULT: <(çağ_Noun)(-)(çağ:noun_S + a3sg_S + ın:p2sg_S + nom_ST)>\n",
      "APPENDING RESULT: <(Çağın_Noun_Prop)(-)(çağın:nounProper_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(ün_Noun)(-)(ün:noun_S + a3sg_S + pnon_S + nom_ST + lü:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(filozof_Noun)(-)(filozof:noun_S + lar:a3pl_S + ın:p2sg_S + dan:abl_ST)>\n",
      "APPENDING RESULT: <(filozof_Noun)(-)(filozof:noun_S + lar:a3pl_S + ı:p3sg_S + ndan:abl_ST)>\n",
      "APPENDING RESULT: <(filozof_Noun)(-)(filozof:noun_S + lar:a3pl_S + ı:p3pl_S + ndan:abl_ST)>\n",
      "APPENDING RESULT: <(biri_Pron_Quant)(-)(biri:pronQuant_S + pQuantA3sg_S + pP3sg_S + pNom_ST + pronZeroDeriv_S + pvVerbRoot_S + pvPresent_S + nA3sg_S + dir:nCop_ST)>\n",
      "APPENDING RESULT: <(bir_Adj)(-)(bir:adjectiveRoot_ST + adjZeroDeriv_S + noun_S + a3sg_S + i:p3sg_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + nA3sg_S + dir:nCop_ST)>\n",
      "APPENDING RESULT: <(bir_Num_Card)(-)(bir:numeralRoot_ST + numZeroDeriv_S + noun_S + a3sg_S + i:p3sg_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + nA3sg_S + dir:nCop_ST)>\n",
      "APPENDING RESULT: <(ün_Noun)(-)(ün:noun_S + a3sg_S + pnon_S + nom_ST + lü:with_S + adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(söz_Noun)(-)(söz:noun_S + ler:a3pl_S + in:p2sg_S + den:abl_ST)>\n",
      "APPENDING RESULT: <(söz_Noun)(-)(söz:noun_S + ler:a3pl_S + i:p3sg_S + nden:abl_ST)>\n",
      "APPENDING RESULT: <(söz_Noun)(-)(söz:noun_S + ler:a3pl_S + i:p3pl_S + nden:abl_ST)>\n",
      "APPENDING RESULT: <(şöyle_Adv)(-)(şöyle:advForVerbDeriv_ST + avZeroToVerb_S + nVerb_S + nPresent_S + nA3sg_S + dir:nCop_ST)>\n",
      "APPENDING RESULT: <(bir_Adj)(-)(bir:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Adv)(-)(bir:advRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Num_Card)(-)(bir:numeralRoot_ST)>\n",
      "APPENDING RESULT: <(bir_Det)(-)(bir:detRoot_ST)>\n",
      "APPENDING RESULT: <(var_Adj)(-)(var:adjectiveRoot_ST)>\n",
      "APPENDING RESULT: <(varmak_Verb)(-)(var:verbRoot_S + vImp_S + vA2sg_ST)>\n",
      "APPENDING RESULT: <(var_Noun)(-)(var:noun_S + a3sg_S + pnon_S + nom_ST)>\n",
      "APPENDING RESULT: <(hiçbir_Det)(-)(hiçbir:detRoot_ST)>\n",
      "APPENDING RESULT: <(bilmek_Verb)(-)(bil:verbRoot_S + me:vNeg_S + diğ:vPastPart_S + noun_S + a3sg_S + im:p1sg_S + nom_ST + nounZeroDeriv_S + nVerb_S + nPresent_S + nA3sg_S + dir:nCop_ST)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ön işleme sonucu metin: ['sokrates', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', 'ünlü', 'sözlerinden', 'şöyledir', \"'bilgiğim\", 'bir', 'var', 'hiçbir', 'bilmediğimdir']\n",
      "Stemming ile kelime kökleri: [('sokrates', ['Sokrates']), ('antik', ['antik']), ('çağın', ['çağmak', 'çağ', 'Çağın']), ('ünlü', ['ün']), ('filozoflarından', ['filozof']), ('biridir', ['bir', 'biri']), ('ünlü', ['ün']), ('sözlerinden', ['söz']), ('şöyledir', ['şöyle']), ('bilgiğim', ['bilgiğim']), ('bir', ['bir']), ('var', ['var', 'varmak']), ('hiçbir', ['hiçbir']), ('bilmediğimdir', ['bilmek'])]\n"
     ]
    }
   ],
   "source": [
    "import zeyrek\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    mystopwords = set(stopwords.words(\"turkish\"))\n",
    "    \n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit() and token not in punctuation]\n",
    "    \n",
    "    return remove_stops_digits(word_tokenize(text))\n",
    "\n",
    "\n",
    "def lemma_words(words):\n",
    "    lemmatizer = zeyrek.MorphAnalyzer()\n",
    "    return [lemmatizer.lemmatize(word)[0] for word in words]\n",
    "\n",
    "\n",
    "text = \"Sokrates, antik çağın ünlü filozoflarından biridir. En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"\n",
    "processed_text = preprocess_text(text)\n",
    "stemmed_words = lemma_words(processed_text)\n",
    "\n",
    "print(\"Ön işleme sonucu metin:\", processed_text)\n",
    "print(\"Stemming ile kelime kökleri:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ön işleme sonucu metin: ['sokrates', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', 'ünlü', 'sözlerinden', 'şöyledir', \"'bilgiğim\", 'bir', 'var', 'hiçbir', 'bilmediğimdir']\n",
      "Stemming ile kelime kökleri: ['sokrates', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', 'ünlü', 'sözlerinden', 'şöyledir', \"'bilgiğim\", 'bir', 'var', 'hiçbir', 'bilmediğimdir']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    mystopwords = set(stopwords.words(\"turkish\"))\n",
    "    \n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit() and token not in punctuation]\n",
    "    \n",
    "    return remove_stops_digits(word_tokenize(text))\n",
    "\n",
    "\n",
    "def lemma_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]\n",
    "\n",
    "\n",
    "text = \"Sokrates, antik çağın ünlü filozoflarından biridir. En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"\n",
    "processed_text = preprocess_text(text)\n",
    "stemmed_words = lemma_words(processed_text)\n",
    "\n",
    "print(\"Ön işleme sonucu metin:\", processed_text)\n",
    "print(\"Stemming ile kelime kökleri:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaynaklar\n",
    "- Natural Language Processing with Python (O’Reilly) - Kitap\n",
    "- Practical Natural Language Processing (O’Reilly) - Kitap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lütfen kaynak belirtmeden bu içeriği tamamen veya kısmen kopyalamayın. Görüş, öneri, şikayet ve işbirliği gibi konular için **kolaydilisleme@gmail.com**_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
