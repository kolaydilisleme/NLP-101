{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lütfen kaynak belirtmeden bu içeriği tamamen veya kısmen kopyalamayın. Görüş, öneri, şikayet ve işbirliği gibi konular için **kolaydilisleme@gmail.com**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizasyon (Tokenleştirme)\n",
    "\n",
    "Metin verileriyle çalışırken işletmemiz gereken en temel adımdır. Token kelime veya duruma göre cümle anlamına gelebilmektedir. Dolayısıyla tokenize etmek derken bir metni **token**larına yani kelimelerine veya cümlelerine ayırma işleminden bahsedilir. Metinler genellikle kelimelerine ayrılır fakat duruma göre cümlelerine ayırmak da iş görebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neden Tokenizasyon Yapılır?\n",
    "\n",
    "- **Dil Yapısını Anlama:** Metin anlamlı parçalarına -temel yapı taşlarına- yani kelimelerine ayrıldığı takdirde dil yapısı daha net anlaşılabilir.\n",
    "\n",
    "- **Frekans Analizi:** Belirli kelimelerin bir metinde ne sıklıkla geçtiğini görmemizi sağlar, bu sayede de metnin genel konusu anlaşılabilir.\n",
    "\n",
    "- **Metin Temizleme:** Gereksiz veya istenmeyen karakterleri (örn: noktalama işaretleri, özel karakterler, bağlaçlar vb.) tespit ederek kaldırmamıza yardımcı olabilir. Bu sayede de metin gereksiz detaylardan daha kolay ve temiz analiz edilebilir hale gelir. \n",
    "\n",
    "- **Özellik Çıkarımı:** Metinden özellik çıkarımı yapmamızı sağlar. Örneğin, belirli kelimelerin veya kelime gruplarının varlığı, bir metnin belirli bir sınıfa ait olup olmadığını belirlememize yardımcı olabilir. Mesela, makine ve öğrenme tek başına bir anlam ifade etmezken birlikte geçtiği durumların sıklığı \"makine öğrenmesi\" terminolojisine ait olduğu bilgisini verir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split Metodu ile Tokenizasyon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizasyon yapmanın birçok farklı yöntemi vardır. Python'da tokenizasyon yapmanın en basit yöntemlerinden biri `split()` fonksiyonudur. Bu fonksiyon ile metni boşluklarına göre tokenlerine ayırabiliriz.\n",
    "\n",
    "Not: `split()` metodu noktalama işaretlerini dikkate almadan tokenize işlemini gerçekleştirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Sokrates, antik çağın ünlü filozoflarından biridir. \n",
    "            En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sokrates,', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir.', 'En', 'ünlü', 'sözlerinden', 'biri', 'şöyledir:', \"'Bilgiğim\", 'bir', 'şey', 'var,', 'o', 'da', 'hiçbir', 'şey', \"bilmediğimdir.'\"]\n"
     ]
    }
   ],
   "source": [
    "# split() fonksiyonu ile tokenizasyon\n",
    "\n",
    "tokens = text.split()\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RegEx ve NLTK ile Tokenizasyon\n",
    "\n",
    "### RegEx \n",
    "[Regex](https://docs.python.org/3/library/re.html) (Regular Expression), yani Düzenli İfadeler, belirli bir deseni (pattern) aramak veya bu deseni kullanarak metin üzerinde işlemler yapmak için kullanılan bir araçtır.\n",
    "\n",
    "Daha gelişmiş bir tokenizasyon işlemi için regex kullanabiliriz. Regex ile belirlediğiniz pattern'e göre metini tokenlerine ayırabiliriz. Örnek kod, noktalama işaretlerini dikkate alarak tokenleştirme işlemini gerçekleştirir.\n",
    "\n",
    "### NLTK Kütüphanesi\n",
    "[NLTK](https://www.nltk.org/) (Natural Language Toolkit) Python için popüler bir NLP kütüphanesidir. Temel kavramları gerçeklemek için sunduğu hazır fonksiyonları kullanmak oldukça pratiktir.\n",
    "\n",
    "NLTK kütüphanesinin regex pattern'lerini destekleyen `nltk.regexp_tokenize(text, pattern)` fonksiyonu da kullanılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Regex : ['Sokrates', ',', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', '.', 'En', 'ünlü', 'sözlerinden', 'biri', 'şöyledir', ':', \"'\", 'Bilgiğim', 'bir', 'şey', 'var', ',', 'o', 'da', 'hiçbir', 'şey', 'bilmediğimdir', '.', \"'\"]\n",
      "\n",
      "Tokens NLTK : ['Sokrates', ',', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', '.', 'En', 'ünlü', 'sözlerinden', 'biri', 'şöyledir', ':', \"'\", 'Bilgiğim', 'bir', 'şey', 'var', ',', 'o', 'da', 'hiçbir', 'şey', 'bilmediğimdir', '.', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "text = \"Sokrates, antik çağın ünlü filozoflarından biridir. En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"\n",
    "\n",
    "pattern = r'''(?x)\n",
    "[A-Z]\\.+\n",
    "| \\w+(?:-\\w+)*\n",
    "| \\$?\\d+(?:\\.\\d+)?%?\n",
    "| \\.\\.\\.\n",
    "| [][.,;\"'?():-_`]\n",
    "'''\n",
    "\n",
    "tokens_regex = re.findall(pattern, text)\n",
    "tokens_nltk = nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "print(\"Tokens Regex :\", tokens_regex)\n",
    "print(\"\\nTokens NLTK :\", tokens_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NLTK ile Hazır Tokenizasyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/enes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizasyonu gerçekleştirmek için **punkt** modelini kullancağız. Punkt, dilbilgisel kuralları ve örüntüleri içeren önceden eğitilmiş bir modeldir. Türkçe dili de dahil olmak üzere bir çok dil için tokenizasyon işlemini gerçekleştirebilmektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Türkçe metin\n",
    "tr_text = \"Sokrates, antik çağın ünlü filozoflarından biridir. En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İngilizce metin\n",
    "en_text = \"Socrates is one of the famous philosophers of the ancient era. One of his most famous quotes is: 'The only thing I know is that I know nothing.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sokrates', ',', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', '.', 'En', 'ünlü', 'sözlerinden', 'biri', 'şöyledir', ':', \"'Bilgiğim\", 'bir', 'şey', 'var', ',', 'o', 'da', 'hiçbir', 'şey', 'bilmediğimdir', '.', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "# Türkçe (word) örnek\n",
    "tr_tokens = word_tokenize(tr_text)\n",
    "print(tr_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sokrates, antik çağın ünlü filozoflarından biridir.', \"En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"]\n"
     ]
    }
   ],
   "source": [
    "# Türkçe (sentence) örnek\n",
    "tr_sent_tokens = sent_tokenize(tr_text)\n",
    "print(tr_sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Socrates', 'is', 'one', 'of', 'the', 'famous', 'philosophers', 'of', 'the', 'ancient', 'era', '.', 'One', 'of', 'his', 'most', 'famous', 'quotes', 'is', ':', \"'The\", 'only', 'thing', 'I', 'know', 'is', 'that', 'I', 'know', 'nothing', '.', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "# İngilizce (word) örnek\n",
    "en_tokens = word_tokenize(en_text)\n",
    "print(en_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yukarıdaki kodların çıktısında görüldüğü üzere noktalama işaretleri ve [stop words]()'ler de token olarak döndürülür. Aşağıdaki fonksiyon ile noktalama işaretleri ve stop words'lerden arındırılmış bir tokenizasyon çıktısı elde edebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sokrates', 'antik', 'çağın', 'ünlü', 'filozoflarından', 'biridir', 'ünlü', 'sözlerinden', 'şöyledir', \"'bilgiğim\", 'bir', 'var', 'hiçbir', 'bilmediğimdir']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words('turkish'))\n",
    "\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit() and token not in punctuation]\n",
    "    \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "text = \"Sokrates, antik çağın ünlü filozoflarından biridir. En ünlü sözlerinden biri şöyledir: 'Bilgiğim bir şey var, o da hiçbir şey bilmediğimdir.'\"\n",
    "preprocessed_text = preprocess_corpus([text])\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['socrates', 'one', 'famous', 'philosophers', 'ancient', 'era', 'one', 'famous', 'quotes', \"'the\", 'thing', 'know', 'know', 'nothing']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def remove_stops_digits(tokens):\n",
    "        return [token.lower() for token in tokens if token.lower() not in mystopwords and not token.isdigit() and token not in punctuation]\n",
    "    \n",
    "    return [remove_stops_digits(word_tokenize(text)) for text in texts]\n",
    "\n",
    "text = \"Socrates is one of the famous philosophers of the ancient era. One of his most famous quotes is: 'The only thing I know is that I know nothing.'\"\n",
    "preprocessed_text = preprocess_corpus([text])\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaynaklar\n",
    "- Natural Language Processing with Python (O’Reilly) - Kitap\n",
    "- Practical Natural Language Processing (O’Reilly) - Kitap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lütfen kaynak belirtmeden bu içeriği tamamen veya kısmen kopyalamayın. Görüş, öneri, şikayet ve işbirliği gibi konular için **kolaydilisleme@gmail.com**_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
